{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystemmer = Mystem()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dic = {}\n",
    "stem_persona_dic = {}\n",
    "\n",
    "def stem_phrase(tokens):\n",
    "    stemmed_phrase = \"\"\n",
    "    for token in tokens:\n",
    "        word = token['text']\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        if 'analysis' in token:\n",
    "            new_stemmed_word = \"\"\n",
    "            for i in range(len(stemmed_word)):\n",
    "                new_stemmed_word += stemmed_word[i].upper() if word[i].isupper() else stemmed_word[i]\n",
    "            stemmed_word = new_stemmed_word\n",
    "        stemmed_phrase += stemmed_word\n",
    "    return stemmed_phrase.rstrip('\\n')\n",
    "\n",
    "with open('collection.txt') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip('\\n').split('\\t')\n",
    "        tag = line[0]\n",
    "        phrase = line[1]\n",
    "        if tag == \"ORG\":\n",
    "            stem_dic[stem_phrase(mystemmer.analyze(phrase))] = \"ORG\"\n",
    "        if tag == \"PER\":\n",
    "            stem_persona_dic[stem_phrase(mystemmer.analyze(phrase))] = \"PERSON\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_capitalised(text):\n",
    "    return len(text) > 0 and text[0].isalpha() and text[0].isupper()\n",
    "\n",
    "def is_word_token(token):\n",
    "    return 'analysis' in token\n",
    "\n",
    "def is_russian_word_token(token):\n",
    "    return is_word_token(token) and token['analysis']\n",
    "\n",
    "def detect_name_abbr(analyzed):\n",
    "    result = []\n",
    "    position = 0\n",
    "    for i, token in enumerate(analyzed):\n",
    "        text = token['text']\n",
    "        if is_word_token(text):\n",
    "            following_token = analyzed[i + 1]\n",
    "            if len(text) == 1 and text.isupper() and following_token['text'][0] == '.':\n",
    "                result.append((position, len(token['text'])))\n",
    "        position = position + len(token['text'])\n",
    "    return result\n",
    "\n",
    "def detect_double_capitalised(analyzed):\n",
    "    result = []\n",
    "    position = 0\n",
    "    \n",
    "    for i, token in enumerate(analyzed):\n",
    "        if 'analysis' in token:\n",
    "            if analyzed[i + 1]['text'] == \" \":\n",
    "                sur_token = analyzed[i + 2]\n",
    "                if is_russian_word_token(token) and is_russian_word_token(sur_token):\n",
    "                    if not is_abbr(token['text']) and not is_abbr(sur_token['text']):\n",
    "                        if is_capitalised(token['text']) and is_capitalised(sur_token['text']):\n",
    "                            if 'S' == token['analysis'][0]['gr'][0] and 'S' == sur_token['analysis'][0]['gr'][0]:\n",
    "                                result.append((position, len(token['text'])))\n",
    "                                result.append((position + len(token['text']) + 1, len(sur_token['text'])))\n",
    "        position = position + len(token['text'])\n",
    "    return result\n",
    "\n",
    "def detect_personas(text):\n",
    "    analysed = mystemmer.analyze(text)\n",
    "    results = detect_from_dict_stem(analysed, stem_persona_dic) \\\n",
    "            + detect_name_abbr(analysed) \\\n",
    "            + detect_double_capitalised(analysed)\n",
    "    return list(set(results))\n",
    "\n",
    "def detect_eng(analysed):\n",
    "    result = []\n",
    "    position = 0\n",
    "    for token in analysed:\n",
    "        if token['text'][0] in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' and len(token['text']) > 1:\n",
    "            result.append((position, len(token['text'])))  \n",
    "        position = position + len(token['text'])\n",
    "    return result\n",
    "\n",
    "def is_abbr(text):\n",
    "    return text.isupper() and text.isalpha() and len(text) > 1\n",
    "\n",
    "def detect_abbr(analysed):\n",
    "    result = []\n",
    "    position = 0\n",
    "    for token in analysed:\n",
    "        if is_abbr(token['text']):\n",
    "            result.append((position, len(token['text'])))  \n",
    "        position = position + len(token['text'])\n",
    "    return result\n",
    "\n",
    "def detect_in_quotes(analysed):\n",
    "    result = []\n",
    "    position = 0\n",
    "    token_num = 0\n",
    "    st = []\n",
    "    \n",
    "    for token in analysed:\n",
    "        for sym in token['text']:\n",
    "            if sym == '«' and token_num != 0:\n",
    "                st.append(token_num)\n",
    "            if sym == '»' and st:\n",
    "                st.pop()\n",
    "        if 'analysis' in token and st:\n",
    "            if analysed[st[-1] + 1]['text'][0].isupper():\n",
    "                result.append((position, len(token['text'])))\n",
    "        position = position + len(token['text'])\n",
    "        token_num += 1\n",
    "    return result\n",
    "\n",
    "def detect_from_dict_stem(analysed, dic):\n",
    "    low = 1\n",
    "    \n",
    "    result = []\n",
    "    texts = []\n",
    "    position = 0\n",
    "    \n",
    "    for cur, token in enumerate(analysed):\n",
    "        if is_word_token(token):\n",
    "            for i in range(low, 10):\n",
    "                if cur + i > len(analysed):\n",
    "                    break\n",
    "                s = stem_phrase(analysed[cur:cur + i])\n",
    "                if s in dic and len(s) > 1:\n",
    "                    texts.append(s)\n",
    "                    cur_position = position\n",
    "                    for j in range(i):\n",
    "                        l = len(analysed[cur + j]['text'])\n",
    "                        if 'analysis' in analysed[cur + j]:\n",
    "                            result.append((cur_position, l))\n",
    "                        cur_position += l\n",
    "                    break\n",
    "        position = position + len(token['text'])\n",
    "    if texts:\n",
    "        print(texts)\n",
    "    return result\n",
    "\n",
    "def detect_orgs(text):\n",
    "    analysed = mystemmer.analyze(text)\n",
    "    results = detect_eng(analysed) + detect_abbr(analysed) \\\n",
    "    + detect_in_quotes(analysed) + detect_from_dict_stem(analysed, stem_dic)# + detect_from_dict(text, bad_dic)\n",
    "    return list(set(results))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(pref, arr):\n",
    "    return ''.join(map(lambda x: \"{} {} {} \".format(x[0], x[1], pref), arr)) \n",
    "\n",
    "total = 0\n",
    "last_line = None\n",
    "try:\n",
    "    with open('dataset.txt', 'r') as data:\n",
    "        with open('answer.txt', 'w') as out:\n",
    "            for line in data:\n",
    "                last_line = line\n",
    "                personas = detect_personas(line)\n",
    "                orgs = detect_orgs(line)\n",
    "                total += len(personas) + len(orgs)\n",
    "                result = to_str(\"PERSON\", personas) + to_str(\"ORG\", orgs) + \"EOL\\n\"\n",
    "                out.write(result)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(last_line)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
