{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "special = ['beg', 'haps', 'sads', 'excl', 'point', 'myst', 'en', 'не']\n",
    "\n",
    "def word_map_punct(word):\n",
    "    happy_smiles = [':)', ';)', ':-)', ':D', ':P', ';-)', '(:', '(-:', '((']\n",
    "    sad_smiles = [':(', ';(', ':-(', 'D:', ';-(', '):', ')-:', '))']\n",
    "    for smile in happy_smiles:\n",
    "        if smile in word:\n",
    "            return 'haps'\n",
    "    for smile in sad_smiles:\n",
    "        if smile in word:\n",
    "            return 'sads'\n",
    "    if '!' in word:\n",
    "        return 'excl'\n",
    "    if '...' in word:\n",
    "        return 'myst'\n",
    "    if '.' in word:\n",
    "        return 'point'\n",
    "    return word\n",
    "\n",
    "def map_punct(line):\n",
    "    return list(map(word_map_punct, line))\n",
    "\n",
    "text_corpus = []\n",
    "lemmatized_text_corpus = []\n",
    "scores = []\n",
    "cnt = 0\n",
    "with open('texts_train.txt', 'r') as texts:\n",
    "    with open('scores_train.txt', 'r') as scores_file:\n",
    "        for line, score in zip(texts, scores_file):\n",
    "            line = line.rstrip('\\n')\n",
    "            score = int(score)\n",
    "            scores.append(score)\n",
    "            \n",
    "            text_corpus.append((line, score))\n",
    "            lemmatized_text_corpus.append((['beg'] + map_punct(m.lemmatize(line)) + ['en'], score))\n",
    "            cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score_stats = {i: 0 for i in range(1, 11)}\n",
    "for score in scores:\n",
    "    score_stats[score] += 1\n",
    "\n",
    "plt.bar(score_stats.keys(), score_stats.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1. Text: В литературном отношении - ниже плинтуса, но почитать имеет смысл, чтобы понять, как устроены мелкие пакостники.\n",
      "Score 2. Text: Абсолютно согласна с nassy. Автор, ты кто? С чего ты взял, что можешь считать свои банальности истиной? желание возвыситься путем принижения остальных?? так это иллюзия высоты, не более того. Ощущение такое, что книгу \"истина в тезисах\" написал другой Мороз, а не тот, кто написал \"Пособие для гениев\". Короче, дрянь книга. А вот \"Пособие для гениев\" - рекомендую\n",
      "Score 3. Text: Хрень полная! Похоже на переложенный на бумагу квест. Ходят, ищут, каакие-то подсказки неимоверные. Короче, поддерживаю amam - УЖОС!\n",
      "Score 4. Text: Довольно бесхребетно и постыло. Однако, некоторые мометы попросту гениальны. Например, сцена с коммивояжером из самого начала.\n",
      "Score 5. Text: Всё бы, может, и ничего, но эти слёзы-надрывы, надрывы-слёзы... На грани психического срыва, чесслово. И как-то слишком растянуто. Моя психика страдала :)\n",
      "Score 6. Text: Смотрите, сейчас я в первый и, возможно, последний раз напишу как настоящий сноб: \"Было, было... Все это было\". Не совсем так, не совсем то, но общая атмосфера тотальной безнадеги времен перестройки, жизни без денег и веры в завтрашний день просто идеальна. Как уже упомяналось, одна из редких книг, в которых подача соответствует материалу. Однако, если брать чисто литературную ценность, без этих \"документальных\" элементов, то книга -- крепкий среднячок.\n",
      "Score 7. Text: Долго \"въезжала\" в главного героя, не скажу чтоб не понравилось - просто как-то очень жестко, видно, что мужчина писал. К Ольге претензий нет - как всегда легкое и захватывающее чтение.\n",
      "Score 8. Text: Книга хороша, неглупа, но очень уж затянута.\n",
      "Score 9. Text: Я читала эту книгу и потихоньку впадала в истерику - мужчинам не понравится, а женщины - перед тем как прочитать ее, морально подготовьте себя, что вы за раз ее прочтете и потом ночь спать не будете, обдумывая все сюжетные перипетии...\n",
      "Score 10. Text: Это первая осознанная книга в моей жизни - я без ума от нее. Лирическая, трагическая история вначале и счастливый конец. В конце я плакала и в моем детском воображении строились катрины несправедливости и одновременно логичности этой жизни. Все будет хорошо, даже если сейчс хуже некуда...\n"
     ]
    }
   ],
   "source": [
    "samples = {}\n",
    "for line, score in text_corpus[:5000]:\n",
    "    samples[score] = line\n",
    "    \n",
    "for i in range(1, 11):\n",
    "    print(\"Score {}. Text: {}\".format(i, samples[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramms = {}\n",
    "bigramms = {}\n",
    "threegramms = {}\n",
    "\n",
    "def filter_words(words):\n",
    "    return list(filter(lambda x: x in special or x[0] in 'абвгдеёжзийклмнопрстуфхцчшщ' \\\n",
    "                       and x not in russian_stopwords, words))\n",
    "\n",
    "def add_to_gramms(storage, key, score):\n",
    "    storage[key] = storage.get(key, [])\n",
    "    storage[key].append(score)\n",
    "\n",
    "for i, (line, score) in enumerate(lemmatized_text_corpus):\n",
    "    line = filter_words(line)\n",
    "    for word in line:\n",
    "        add_to_gramms(unigramms, word, score)\n",
    "    for word1, word2 in zip(line, line[1:]):\n",
    "        add_to_gramms(bigramms, (word1, word2), score)\n",
    "    for word1, word2, word3 in zip(line, line[1:], line[2:]):\n",
    "        add_to_gramms(threegramms, (word1, word2, word3), score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609147\n",
      "('очень', 'понравиться', 'point')\n",
      "('beg', 'очень', 'понравиться')\n",
      "('beg', 'отличный', 'книга')\n",
      "('понравиться', 'point', 'en')\n",
      "('beg', 'хороший', 'книга')\n",
      "('т', 'point', 'д')\n",
      "('point', 'д', 'point')\n",
      "('beg', 'отличный', 'фильм')\n",
      "('не', 'понравиться', 'point')\n",
      "('не', 'point', 'en')\n",
      "('beg', 'очень', 'интересный')\n",
      "('point', 'очень', 'понравиться')\n",
      "('point', 'главный', 'герой')\n",
      "('рекомендовать', 'point', 'en')\n",
      "('point', 'не', 'знать')\n",
      "('beg', 'хороший', 'фильм')\n"
     ]
    }
   ],
   "source": [
    "print(len(threegramms))\n",
    "print(*dict(sorted(threegramms.items(), key=lambda x: len(x[1]), reverse=True)[4:20]), sep='\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_dict = {}\n",
    "\n",
    "with open('words_all_full_rating.csv', encoding='cp1251', newline='') as csvfile:\n",
    "    words = csv.reader(csvfile, delimiter=';')\n",
    "    next(words)\n",
    "    for row in words:\n",
    "        mean = float(row[1].replace(',', '.'))\n",
    "        d    = float(row[2].replace(',', '.'))\n",
    "        result = int(row[3])\n",
    "        if result != 0 and d < 0.5 and abs(mean) > 1:\n",
    "            tone_dict[row[0]] = {'mean' : mean, 'd' : d, 'res' : result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 22021 46274\n"
     ]
    }
   ],
   "source": [
    "def filter_n_grams(ngrams):\n",
    "    filtered_ngrams = {}\n",
    "\n",
    "    for key in ngrams:\n",
    "        word_in = key in tone_dict\n",
    "        for word in key:\n",
    "            if word in tone_dict:\n",
    "                word_in = True\n",
    "        if word_in:\n",
    "            filtered_ngrams[key] = ngrams[key]\n",
    "    return filtered_ngrams\n",
    "\n",
    "filtered_bigrams = filter_n_grams(bigramms)\n",
    "filtered_threegrams = filter_n_grams(threegramms)\n",
    "filtered_unigrams = filter_n_grams(unigramms)\n",
    "\n",
    "print(*map(len, [filtered_unigrams, filtered_bigrams, filtered_threegrams]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('наркота', 'вера'), ('удивление', 'гармонично'), ('гармонично', 'вплетать'), ('почему', 'жестокий'), ('жестокий', 'добрый'), ('добрый', 'система'), ('предательство', 'подлость'), ('любимый', 'даша'), ('приятно', 'несмотря'), ('конец', 'зло'), ('зло', 'автор'), ('правда', 'ляп'), ('ляп', 'вроде'), ('который', 'погребать'), ('погребать', 'миллион'), ('сюжет', 'роскошный'), ('роскошный', 'изложение'), ('захватывать', 'будить'), ('проза', 'добрый'), ('добрый', 'слишком'), ('мужество', 'все-таки'), ('невыносимый', 'чтение'), ('преступление', 'поступок'), ('пелевин', 'захватывать'), ('захватывать', 'поглощать'), ('нестареющий', 'бессмертный'), ('бессмертный', 'произведение'), ('глупо', 'претензия'), ('excl', 'стыдно'), ('стыдно', 'осознавать')])\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted(filtered_bigrams.items(), key=lambda x: len(x[1]), reverse=True)[6000:6030]).keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46274\n"
     ]
    }
   ],
   "source": [
    "filtered_threegrams = {}\n",
    "\n",
    "for key in threegramms:\n",
    "    word_in = False\n",
    "    for word in key:\n",
    "        if word in tone_dict:\n",
    "            word_in = True\n",
    "    if word_in:\n",
    "        filtered_threegrams[key] = threegramms[key]\n",
    "\n",
    "print(len(filtered_threegrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('считать', 'самый', 'любимый'), ('немного', 'грустный', 'впечатлять'), ('грустный', 'впечатлять', 'point'), ('потрясать', 'книга', 'захватывать'), ('книга', 'захватывать', 'волнительный'), ('захватывать', 'волнительный', 'point'), ('особенно', 'не', 'заморачиваться'), ('не', 'заморачиваться', 'несоответствие'), ('заморачиваться', 'несоответствие', 'современный'), ('интересный', 'очень', 'добрый'), ('снова', 'снова', 'удовольствие'), ('снова', 'удовольствие', 'point'), ('удовольствие', 'point', 'обеспечивать'), ('beg', 'захватывать', 'книга'), ('захватывать', 'книга', 'дочитывать'), ('узнавать', 'интересный', 'момент'), ('интересный', 'момент', 'мелочь'), ('beg', 'очень', 'жестокий'), ('защищать', 'myst', 'плохой'), ('вовсе', 'не', 'надежда'), ('не', 'надежда', 'написать'), ('надежда', 'написать', 'средний'), ('некий', 'ментальный', 'бомба'), ('ментальный', 'бомба', 'point'), ('бомба', 'point', 'любой'), ('разварачиваться', 'настоящий', 'трагедия'), ('настоящий', 'трагедия', 'point'), ('не', 'виноватый', 'каждый'), ('виноватый', 'каждый', 'свой'), ('получать', 'удовольствие', 'прочтение')])\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted(filtered_threegrams.items(), key=lambda x: len(x[1]), reverse=True)[14000:14030]).keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(dict(filter(lambda x: x[1]['d'] > 1, sorted(tone_dict.items(), key=lambda x: x[1]['res']))), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as file:\n",
    "    with open('answer.txt', 'w') as out:\n",
    "        for line in file:\n",
    "            line = filter_words(m.lemmatize(line))\n",
    "            tones = []\n",
    "            for prev, word in zip([None] + line, line):\n",
    "                if word not in tone_dict:\n",
    "                    continue\n",
    "                if prev == 'не':\n",
    "                    tones.append(-tone_dict[word]['mean'])\n",
    "                else:\n",
    "                    tones.append(tone_dict[word]['mean'])\n",
    "            result = sum(tones) / len(tones) * 5 + 5  if tones else 7\n",
    "            result = max(min(10, int(result)), 1)\n",
    "            out.write(\"{}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as file:\n",
    "    with open('answer.txt', 'w') as out:\n",
    "        for line in file:\n",
    "            line = filter_words(m.lemmatize(line))\n",
    "            tones = []\n",
    "            for trio in zip(line, line[1:], line[2:]):\n",
    "                if trio not in filtered_threegrams:\n",
    "                    continue\n",
    "                tones.append(sum(filtered_threegrams[trio]) / len(filtered_threegrams[trio]))\n",
    "            if (tones):\n",
    "                result = (sum(tones) / len(tones)) if tones else 5\n",
    "                result = max(min(10, round(result)), 1)\n",
    "                out.write(\"{}\\n\".format(result))\n",
    "                continue\n",
    "            for duo in zip(line, line[1:]):\n",
    "                if duo not in filtered_bigrams:\n",
    "                    continue\n",
    "                tones.append(sum(filtered_bigrams[duo]) / len(filtered_bigrams[duo]))\n",
    "            if tones:\n",
    "                result = (sum(tones) / len(tones)) if tones else 5\n",
    "                result = max(min(10, round(result)), 1)\n",
    "                out.write(\"{}\\n\".format(result))\n",
    "                continue\n",
    "            for word in line:\n",
    "                if (word in unigramms):\n",
    "                    tones.append(sum(unigramms[word]) / len(unigramms[word]))\n",
    "\n",
    "            result = (sum(tones) / len(tones)) if tones else 7\n",
    "            result = max(min(10, round(result)), 1)\n",
    "            out.write(\"{}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for line, _ in lemmatized_text_corpus:\n",
    "    line = filter_words(line)\n",
    "    res = []\n",
    "    for prev, word in zip([None] + line, line):\n",
    "        if word == 'не':\n",
    "            continue\n",
    "        if prev == 'не':\n",
    "            res.append(\"NOT_\" + word)\n",
    "        else:\n",
    "            res.append(word)\n",
    "    corpus.append(' '.join(res))\n",
    "split_point = 19800\n",
    "    \n",
    "train, train_scores = corpus[:split_point], scores[:split_point]\n",
    "test, test_scores = corpus[split_point:], scores[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19800, 23933) ['beg', 'beg en', 'beg excl', 'beg haps', 'beg myst', 'beg not_бывать', 'beg not_видеть', 'beg not_впечатлить', 'beg not_говорить', 'beg not_детектив']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape, vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=10, n_estimators=1000)\n",
    "regr.fit(X, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1e-05, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso(max_iter=100000, alpha=0.00001)\n",
    "clf.fit(X, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-3dd13d1789a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m std = np.std([tree.feature_importances_ for tree in regr.estimators_],\n\u001b[1;32m      3\u001b[0m              axis=0)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Print the feature ranking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importances = regr.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in regr.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = vectorizer.transform(test)\n",
    "regr.score(test_X, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23124678973668666"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as file:\n",
    "    with open('answer.txt', 'w') as out:\n",
    "        qs = []\n",
    "        for line in file:\n",
    "            line = filter_words(['beg'] + map_punct(m.lemmatize(line)) + ['en'])\n",
    "            res = []\n",
    "            for prev, word in zip([None] + line, line):\n",
    "                if word == 'не':\n",
    "                    continue\n",
    "                if prev == 'не':\n",
    "                    res.append(\"NOT_\" + word)\n",
    "                else:\n",
    "                    res.append(word)\n",
    "            qs.append(' '.join(res))\n",
    "        q_X = vectorizer.transform(qs)\n",
    "        ys = clf.predict(q_X)\n",
    "        \n",
    "        for y in ys:\n",
    "            result = round(y / 10 * 12 - 1)\n",
    "            result = max(min(10, int(result)), 1)\n",
    "            out.write(\"{}\\n\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
